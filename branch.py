import torch.nn as nn
import torch.nn.functional as F

class SlowBranch(nn.Module):
    """
    Paper-equivalent slow branch:
    FC -> 4xGRU(64) -> FC -> epsilon_S
    """
    def __init__(self, input_len, gru_hidden=64, gru_layers=4, out_dim=32):
        super().__init__()
        self.fc_in = nn.Linear(input_len, gru_hidden)

        self.gru = nn.GRU(
            input_size=gru_hidden,
            hidden_size=gru_hidden,
            num_layers=gru_layers,
            batch_first=True
        )

        self.fc_out = nn.Linear(gru_hidden, out_dim)

    def forward(self, x):
        """
        x: (B, N_slow, L_S)
        """
        B, N, L = x.shape
        x = self.fc_in(x)           # (B, N, 64)
        x = F.relu(x)

        x, _ = self.gru(x)          # (B, N, 64)
        x = self.fc_out(x)          # (B, N, out_dim)
        return x

class SSMMFastBranch(nn.Module):
    """
    SSMM Fast Branch:
    FIN:  x -> hidden
    FOUT: hidden -> x
    A, g: generated by slow branch
    """
    def __init__(self, hidden_dim, frame_len):
        super().__init__()
        self.hidden_dim = hidden_dim

        self.f_in = nn.Linear(frame_len, hidden_dim, bias=False)
        self.f_out = nn.Linear(hidden_dim, frame_len, bias=False)

    def forward(self, x_f, A, g):
        """
        x_f: (B, L_F)
        A:   (B, H)
        g:   (B, H)
        """
        x_emb = self.f_in(x_f) * g     # (B, H)
        h = A * x_emb                  # NOTE: recurrent state is handled outside
        y = self.f_out(h)              # (B, L_F)
        return y